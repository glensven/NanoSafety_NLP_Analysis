{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use multiple Scopus Author IDs to retrieve lists of articles by author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from config import api_key\n",
    "from pandas.io.json import json_normalize  \n",
    "import nltk\n",
    "import re\n",
    "import io\n",
    "from nltk.corpus import stopwords, reuters\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud\n",
    "import panel as pn\n",
    "import panel.widgets \n",
    "from pathlib import Path\n",
    "from panel.interact import interact\n",
    "import hvplot.pandas\n",
    "import param\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "pn.extension()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare necessary parameters for Scopus request API search tool\n",
    "url = \"http://api.elsevier.com/content/search/scopus?\" + \"?format=json\"\n",
    "headers = {\"X-ELS-APIKey\": api_key, 'Accept':'application/json'}\n",
    "view = \"Complete\"\n",
    "field = [\"dc:description\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Declare the timeframe to search by start and end years\n",
    "# Year is the lowest granularity for this search function\n",
    "start_year = 2009\n",
    "end_year = 2010\n",
    "if start_year == end_year:\n",
    "    date = str(start_year)\n",
    "else: \n",
    "    date = str(start_year) + \"-\" + str(end_year)\n",
    "\n",
    "# Generate a lists of dates for the requested timeframe\n",
    "date_list = []\n",
    "for date in range(start_year, end_year + 1):\n",
    "    date_list.append(int(date))\n",
    "    \n",
    "#date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input the query search term\n",
    "query = \"graphene\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3885, 7030]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test search function to determine the total results from the Get Request for the Specified Date Range\n",
    "total_results = []\n",
    "date_list = []\n",
    "for date in range(start_year, end_year + 1):\n",
    "    date_list.append(int(date))\n",
    "    \n",
    "def total_results_funt(date_list):\n",
    "    for i in date_list:\n",
    "        parameters = {\"query\": query, \"view\": view, \"date\": str(i), \"count\": 25, \"start\": 0}\n",
    "        article_response = requests.get(url, headers=headers, params=parameters)\n",
    "        article_response_json = article_response.json()\n",
    "        total_results.append(int(article_response_json['search-results']['opensearch:totalResults']))\n",
    "    return total_results\n",
    "\n",
    "total_results = total_results_funt(date_list)\n",
    "total_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search request function \n",
    "# Scopus request has a maximum of 5000 articles returned per year\n",
    "def search_request_funt(query, start_year, end_year):\n",
    "    scopus_search_appended_df = pd.DataFrame()\n",
    "    for year in range (start_year, end_year + 1):\n",
    "        parameters = {\"query\": query, \"view\": view, \"field\": field, \"date\": str(year), \"start\": 0}\n",
    "        article_response = requests.get(url, headers=headers, params=parameters)\n",
    "        article_response_json = article_response.json()\n",
    "        total_results = int(article_response_json['search-results']['opensearch:totalResults'])\n",
    "        for n in range(0, total_results, 25):\n",
    "            parameters = {\"query\": query, \"view\": view, \"date\": str(year), \"count\": 25, \"start\": n}\n",
    "            article_response = requests.get(url, headers=headers, params=parameters)\n",
    "            article_response_json = article_response.json()\n",
    "            scopus_articles_df = pd.DataFrame.from_dict(json_normalize(article_response_json, meta=[\"search-results\"], record_path=[\"search-results\", \"entry\"]), orient=\"columns\")\n",
    "            date_title_description_df = scopus_articles_df[[\"prism:coverDate\", \"dc:title\", \"dc:description\"]]\n",
    "            date_title_description_df.columns = ['Date','Title','Content']\n",
    "            pd.to_datetime(date_title_description_df['Date'], format = \"%Y-%m-%d\")\n",
    "            date_title_description_df.sort_values(by='Date')\n",
    "            date_title_description_df = date_title_description_df.set_index('Date')\n",
    "            scopus_search_appended_df = scopus_search_appended_df.append(date_title_description_df)\n",
    "    return scopus_search_appended_df\n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'search-results'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-a08608028d29>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Generate DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'display.max_rows'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mscopus_search_appended_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msearch_request_funt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_year\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_year\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m# scopus_search_appended_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-37-a8e6d6a032ad>\u001b[0m in \u001b[0;36msearch_request_funt\u001b[1;34m(query, start_year, end_year)\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0marticle_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0marticle_response_json\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marticle_response\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m             \u001b[0mscopus_articles_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_normalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle_response_json\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"search-results\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecord_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"search-results\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"entry\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morient\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"columns\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m             \u001b[0mdate_title_description_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscopus_articles_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"prism:coverDate\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"dc:title\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"dc:description\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mdate_title_description_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Date'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Title'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Content'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pyvizenv\\lib\\site-packages\\pandas\\io\\json\\normalize.py\u001b[0m in \u001b[0;36mjson_normalize\u001b[1;34m(data, record_path, meta, meta_prefix, record_prefix, errors, sep)\u001b[0m\n\u001b[0;32m    265\u001b[0m                 \u001b[0mrecords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m     \u001b[0m_recursive_extract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecord_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pyvizenv\\lib\\site-packages\\pandas\\io\\json\\normalize.py\u001b[0m in \u001b[0;36m_recursive_extract\u001b[1;34m(data, path, seen_meta, level)\u001b[0m\n\u001b[0;32m    236\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmeta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeta_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mlevel\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 238\u001b[1;33m                         \u001b[0mseen_meta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_pull_field\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    239\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m                 _recursive_extract(obj[path[0]], path[1:],\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pyvizenv\\lib\\site-packages\\pandas\\io\\json\\normalize.py\u001b[0m in \u001b[0;36m_pull_field\u001b[1;34m(js, spec)\u001b[0m\n\u001b[0;32m    187\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfield\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'search-results'"
     ]
    }
   ],
   "source": [
    "# Generate DataFrame\n",
    "pd.set_option('display.max_rows', None)\n",
    "scopus_search_appended_df = search_request_funt(query, start_year, end_year)\n",
    "# scopus_search_appended_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save a csv of the dataframe\n",
    "scopus_search_appended_df.to_csv(\"Scopus_Search_\" + \"_\" + query + \"_\" + str(start_year) + \"_\" + str(end_year) + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0]\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set file path\n",
    "scopus_search_path = Path(\"Scopus_Search_\" + \"_\" + query + \"_\" + str(start_year) + \"_\" + str(end_year) + \".csv\", header = 0)\n",
    "# Read CSV to DataFrame\n",
    "query_df = pd.read_csv(scopus_search_path)\n",
    "query_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date Range Slider\n",
    "year_slider_1 = pn.widgets.IntRangeSlider(name='Timeframe', width=300, start=1990, end=2021, value=(2005, 2021))\n",
    "\n",
    "@pn.depends(year_slider)\n",
    "def freq_plot_funt(year_slider):\n",
    "    total_results = []\n",
    "    date_list = []\n",
    "    for date in range(year_slider[0], year_slider[1] + 1):\n",
    "        date_list.append(int(date))\n",
    "    for i in date_list:\n",
    "        parameters = {\"query\": query, \"view\": view, \"date\": str(i), \"count\": 25, \"start\": 0}\n",
    "        article_response = requests.get(url, headers=headers, params=parameters)\n",
    "        article_response_json = article_response.json()\n",
    "        total_results.append(int(article_response_json['search-results']['opensearch:totalResults']))\n",
    "    # Create an annual frequency bar plot for query search\n",
    "    frequency_df = pd.DataFrame([date_list, total_results]).T\n",
    "    frequency_df.columns = ['Year', 'Total Count per Year']\n",
    "    frequency_csv = frequency_df.to_csv(\"Frequency_\" + \"_\" + query + \"_\" + str(year_slider[0]) + \"_\" + str(year_slider[1]) + \".csv\")\n",
    "    return frequency_csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date Range Slider\n",
    "year_slider = pn.widgets.IntRangeSlider(name='Timeframe', width=300, start=1990, end=2021, value=(2010, 2021))\n",
    "\n",
    "@pn.depends(year_slider)\n",
    "def freq_plot_funt(year_slider):\n",
    "    total_results = []\n",
    "    date_list = []\n",
    "    for date in range(year_slider[0], year_slider[1] + 1):\n",
    "        date_list.append(int(date))\n",
    "    for i in date_list:\n",
    "        parameters = {\"query\": query, \"view\": view, \"date\": str(i), \"count\": 25, \"start\": 0}\n",
    "        article_response = requests.get(url, headers=headers, params=parameters)\n",
    "        article_response_json = article_response.json()\n",
    "        total_results.append(int(article_response_json['search-results']['opensearch:totalResults']))\n",
    "    # Create an annual frequency bar plot for query search\n",
    "    frequency_df = pd.DataFrame([date_list, total_results]).T\n",
    "    frequency_df.columns = ['Year', 'Total Count per Year']\n",
    "    plot = frequency_df.hvplot.line(title= \"Total Count per Year for the Word '\" + query + \"' used in Academic Articles\", \n",
    "                                x = \"Year\", \n",
    "                                y = 'Total Count per Year',\n",
    "                                invert = False, \n",
    "                                height = 400,\n",
    "                                width = 800\n",
    "                                )\n",
    "    return plot\n",
    "freq_plot = pn.Column(pn.Row(pn.WidgetBox(year_slider), pn.Column(freq_plot_funt)))\n",
    "#freq_plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y value multiselectors\n",
    "column_selector = pn.widgets.Select(name='Select for a Title or Abstract', value = 'Title', options=['Title', 'Content'])\n",
    "\n",
    "# Word Count Slider\n",
    "word_slider = pn.widgets.IntSlider(name='Max Words', start=5, end=50, step=1, value=15)\n",
    "\n",
    "# Stop word addition\n",
    "text_input = pn.widgets.TextInput(name='Enter Additional Words to Ignore', placeholder='Type Words Here, Seperated by a Space')\n",
    "# Dynamic Plots\n",
    "\n",
    "# Convert dataframe Title column to string text\n",
    "@pn.depends(column_selector, word_slider, text_input)\n",
    "def dataframe_to_string(column_selector, word_slider, text_input):\n",
    "    if column_selector == 'Content':\n",
    "        for row in query_df:\n",
    "            big_string = ''.join(str(query_df['Content']))\n",
    "        sw = set(stopwords.words('english'))\n",
    "        regex = re.compile(\"[^a-zA-Z ]\")\n",
    "        wordlist = re.sub(\"[^\\w]\", \" \",  text_input).split()\n",
    "        sw_addons = {'using', 'via', 'based', 'nan', 'date', 'used', 'b', 'tio', 'nanote'}\n",
    "        sw_addons.update(wordlist)\n",
    "        re_clean = regex.sub('', big_string)\n",
    "        words = word_tokenize(re_clean)\n",
    "        lem = [lemmatizer.lemmatize(word) for word in words]\n",
    "        output = [word.lower() for word in lem if word.lower() not in sw.union(sw_addons)]\n",
    "        full_string = ' '.join(output)\n",
    "        wc_content = WordCloud(width=800, height=600, background_color=\"white\", max_words= word_slider).generate(full_string)\n",
    "        image_1 = wc_content.to_image()\n",
    "        return image_1\n",
    "    else:\n",
    "        for row in query_df:\n",
    "            big_string = ''.join(str(query_df['Title']))\n",
    "        sw = set(stopwords.words('english'))\n",
    "        regex = re.compile(\"[^a-zA-Z ]\")\n",
    "        wordlist = re.sub(\"[^\\w]\", \" \",  text_input).split()\n",
    "        sw_addons = {'using', 'via', 'based', 'nan', 'date', 'used', 'b', 'tio', 'nanote'}\n",
    "        sw_addons.update(wordlist)\n",
    "        re_clean = regex.sub('', big_string)\n",
    "        words = word_tokenize(re_clean)\n",
    "        lem = [lemmatizer.lemmatize(word) for word in words]\n",
    "        output = [word.lower() for word in lem if word.lower() not in sw.union(sw_addons)]\n",
    "        full_string = ' '.join(output)\n",
    "        wc_content = WordCloud(width=800, height=600, background_color=\"white\", max_words= word_slider).generate(full_string)\n",
    "        image_2 = wc_content.to_image()\n",
    "        return image_2\n",
    "\n",
    "word_cloud_tab = pn.Column('# Word Cloud', pn.Row(pn.WidgetBox(column_selector, word_slider, text_input), pn.Column(dataframe_to_string)), background='#f0f0f0')\n",
    "freq_plot_tab = pn.Column('# Frequency Plot', pn.WidgetBox(year_slider), freq_plot_funt, background='#f0f0f0')\n",
    "all_tabs = pn.Tabs(('Frequency Plot', freq_plot_tab), ('Word Cloud', word_cloud_tab))\n",
    "all_tabs.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
